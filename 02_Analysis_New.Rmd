---
title: "Social Status, Liberalization and Attitudes toward Redistribution"
output:
  html_document:
    df_print: paged
---

"Social Status, Liberalization and Attitudes toward Redistribution"

##### ISSP, Social Inequality, Cumulation (1987, 1992, 1999 & 2009)
##### ISSP, Environment, 1993, 2000 & 2010
$~$
$~$

#### **Nate Breznau, Lisa Heukamp & Hung H.V. Nguyen**
#### **University of Bremen**

_____________________________________________________________________________

$~$
$~$

Working paper available here: https://docs.google.com/document/d/1mST25rKYZ5wChRj5WyhxH_65RUa9MNR2OsO5B4-qcwY/edit#

Github repo: https://github.com/hungnguyen167/Social-Status



```{r, include = F}
rm(list = ls())
```


```{r setup, include=FALSE}
library(pacman)
pacman::p_load("tidyverse","knitr","car","expss","stargazer","lavaan","kableExtra","MASS","glm.predict","brant", "semTools","ggplot2","countrycode","rjags", "ragg")

knitr::opts_chunk$set(message = F, warning = F)

```



## Load Data

The data cleaning is done in the file "01_Data Cleaning.Rmd"

```{r loaddata}
load(here::here("data","socstat.Rda"))
```


# Descriptives

## Table 1. Cases by Country-Wave

```{r tbl1, include = T, echo = T}
# Run this if you still have the old socstat version
# df$female <- ifelse(df$female == 8, NA, df$female)



# Cross-tab years and countries
options(knitr.kable.NA = '')
tab1 <- df %>%
  dplyr::group_by(country, wave) %>%
  dplyr::summarize(n=n()) %>%
  spread(wave, n) %>%
  kable(., col.names = c("Country", "SI_1987", "SI_1992", "EN_1993", "SI_1999", "EN_2000", "SI_2009", "EN_2010"), caption = "Country Sample Size by Wave, ISSP 'Social Inequality (SI) & Environment (EN)", format.args = list(big.mark = ","))

kable_styling(tab1)




```



## Table 2. Descriptive Statistics

Final coding 

**wrkst**

1 Employed, full-time
2 Employed, part-time
3 Employed, less than part-time
5 Unemployed
6 Student, school, vocational training
7 Retired
8 Housewife,-man, home duties or Helping family member
9 Permanently disabled
10 Other, not in labour force, or refused, don't know, no answer

**sh_groups

1 "Capital accumulators" 
2 "Mixed service functionaires" 
3 "Blue and lower white collar" 
4 "Socio-cultural (semi-)professionals" 
5 "Low service functionaires"

```{r tbl2, echo = T}

# make categories out of categorical variables
df <- df %>%
  mutate(egp_un = as.numeric(egp6 == "Unskilled"),
         egp_skl = as.numeric(egp6 == "Skilled"),
         egp_othr = as.numeric(egp6 == "Self"),
         egp_rou = as.numeric(egp6 == "Routine"),
         egp_svc2 = as.numeric(egp6 == "Service II"),
         egp_svc1 = as.numeric(egp6 == "Service I"),
         wk_full = as.numeric(wrkst == 1),
         wk_part = as.numeric(wrkst == 2),
         wk_lesspart = as.numeric(wrkst == 3),
         wk_unemp = as.numeric(wrkst == 5),
         wk_school = as.numeric(wrkst == 6),
         wk_retired = as.numeric(wrkst == 7),
         wk_home = as.numeric(wrkst == 8),
         wk_disab = as.numeric(wrkst == 9),
         wk_other = as.numeric(wrkst == 10))

tab2 <- df

tab2 <- dplyr::select(tab2, year, ageC, female, educyrs, child, married, rincZ, hincZ, isei, egp_un, egp_skl, egp_othr, egp_rou, egp_svc2, egp_svc1, sh_groups_1:sh_groups_5, sh_groups_new_1:sh_groups_new_5, wk_full:wk_other, wrkhrs, union, sparent, reduce)

# Get summary stats
tab2_1 <- summarise_all(tab2, list(mean), na.rm = T)
tab2_1.1 <- summarise_all(tab2, list(sd), na.rm = T)
tab2_2 <- summarise_all(tab2, list(min), na.rm = T)
tab2_3 <- summarise_all(tab2, list(max), na.rm = T)
tab2_4 <- summarise_all(tab2, funs(sum(!is.na(.))))
# Transpose into columns
tab2_1 <- as.data.frame(t(round(tab2_1, 2)))
tab2_1.1 <- as.data.frame(t(round(tab2_1.1, 2)))
tab2_2 <- as.data.frame(t(round(tab2_2, 0)))
tab2_3 <- as.data.frame(t(round(tab2_3, 0)))
tab2_4 <- as.data.frame(t(tab2_4))
tab2_0 <- as.data.frame(c("Year","Age, in years", "Female", "Education, in years", "Children", "Married", "Indiv. Earnings, country-Z", "Household Earnings, country-Z", "Occ. Status, ISEI", "EGP - Unskilled", "EGP - Skilled", "EGP - Self/Other", "EGP - Routine", "EGP - Service II", "EGP - Service I", "SH1 - Capital Accumulator", "SH2 - Mixed Service", "SH3 - Blue-collar/low white", "SH4 - SC Semi-Professional", "SH5 - Lower Service", "SH1_new - Capital Accumulator", "SH2_new - Mixed Service", "SH3_new - Blue-collar/low white", "SH4_new - SC Semi-Professional", "SH5_new - Lower Service","Work - Full-time", "Work - Part-Time", "Work - Less-part-time", "Work - Unemployed", "Work - School/Training", "Work - Retired", "Work - Homecare/Family help", "Work - Disabled", "Work - Other/NA", "Hours Worked", "Union", "Parent", "DV - Support Redistribution"))
# Bind into table
tab2f <- cbind(tab2_0,tab2_1,tab2_1.1,tab2_2,tab2_3,tab2_4)
colnames(tab2f) <- c("Variable","Mean","SD","Min","Max","N")
tab2f <- kable(tab2f)
kable_styling(tab2f)
rm(tab2_0, tab2_1, tab2_1.1, tab2_2, tab2_3, tab2_4)
```

## Fig 1. Scatterplot of Preferences for Redistribution

```{r fig1, echo = T}

# make a label variable
df$lab <- countrycode(df$cntry, "iso3n", "iso2c")
df$lab <- paste0(df$lab,df$year)

fig1 <- df %>%
  group_by(cyear) %>%
  mutate(reduceM = mean(reduce, na.rm = T),
         reduceSE = (sd(reduce, na.rm = T)/sqrt(length(which(!is.na(reduce))))))


fig1 <- dplyr::select(fig1, year, cyear, lab, reduceM, reduceSE, wave, cntry)

fig1 <- aggregate(fig1,  by=list(fig1$lab), FUN=mean, na.rm=T)
colnames(fig1) <- c("lab","year","cyear","skip","reduceM","reduceSE","wave","cntry")

fig1$country <- countrycode(fig1$cntry, "iso3n", "country.name")

# only label countries that are in the survey often)
cntry2 <- as.list(unique(fig1$cntry[fig1$year < 1992]))
fig1$country <- ifelse(fig1$cntry  %in% cntry2, fig1$country, NA)

# drop NA countries
fig1a <- subset(fig1, !is.na(country))
agg_png(file = "Fig1.png", width = 1200, height = 600, res = 144)
ggplot(fig1a, aes(x=year, y=reduceM, color=country)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time") +
    ylab(label = "Mean Support")
invisible(dev.off())
knitr::include_graphics("Fig1.png")

```

## Table 3. Correlations, Pooled

```{r corrs}
tab2 <- dplyr::select(tab2, reduce, everything())
cor1 <- cor(tab2, use = "pairwise.complete.obs")
cor1k <- kable(cor1, digits = 2)
kable_styling(cor1k)
# check that correlations look 'right'
```


# Group Comparisons


1 "Capital accumulators" 
2 "Mixed service functionaires" 
3 "Blue and lower white collar" 
4 "Socio-cultural (semi-)professionals" 
5 "Low service functionaires"

### Fig 2. Germany


```{r f2de}

fig2 <- df %>%
  group_by(cyear, sh_groups_new) %>%
    mutate(reduceM = mean(reduce, na.rm = T),
           reduceSE = (sd(reduce, na.rm = T)/sqrt(length(which(!is.na(reduce))))))

fig2 <- dplyr::select(fig2, year, cyear, lab, reduceM, reduceSE, wave, cntry, sh_groups_new)

fig2 <- aggregate(fig2,  by=list(fig2$cyear,fig2$sh_groups_new), FUN=mean, na.rm=T)

fig2de <- subset(fig2, cntry == 276)
fig2de$sh_groups_new <- as.factor(fig2de$sh_groups_new)
agg_png("Fig2.png", width = 1000, height = 600, res = 144)
ggplot(fig2de, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Germany") +
    ylab(label = "Mean Support") +
    scale_color_hue(labels = c("Capital Accum.", "Mixed Srvc.", "Blue-collar/low", "Soc.Cult. Prof.", "Low Srvc."))
invisible(dev.off())
knitr::include_graphics("Fig2.png")
```


```{r f2de_gender}

fig2g <- df %>%
  group_by(cyear, sh_groups_new, female) %>%
    mutate(reduceM = mean(reduce, na.rm = T),
           reduceSE = (sd(reduce, na.rm = T)/sqrt(length(which(!is.na(reduce))))))

fig2g <- dplyr::select(fig2g, year, cyear, lab, reduceM, reduceSE, wave, cntry, sh_groups_new, female, ageC)
fig2g$sh_groups_new_g <- fig2g$sh_groups_new + (5*fig2g$female)
fig2g <- subset(fig2g, select = -c(sh_groups_new, female))

fig2g <- aggregate(fig2g,  by=list(fig2g$cyear,fig2g$sh_groups_new_g), FUN=mean, na.rm=T)

fig2gde <- subset(fig2g, cntry == 276)
fig2gde$sh_groups_new <- as.factor(fig2gde$sh_groups_new)



# Fig 3
agg_png(file = "Fig3.png", width = 1200, height = 600, res = 144)
ggplot(fig2gde, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Germany") +
    ylab(label = "Mean Support") +
    scale_color_manual(labels = c("M: Capital Accum.", "M: Mixed Srvc.", "M: Blue-collar/low", "M: Soc.Cult. Prof.", "M: Low Srvc.", "F: Capital Accum.", "F: Mixed Srvc.", "F: Blue-collar/low", "F: Soc.Cult. Prof.", "F: Low Srvc."), values = c("red", "darkorange", "royalblue", "purple", "green", "red3", "darkorange3", "royalblue4", "purple3", "green4"))
invisible(dev.off())
knitr::include_graphics("Fig3.png")


# Fig 7 
fig7 <- fig2gde[which(fig2gde$sh_groups_new %in% c(1,5,6,10)),]
agg_png("Fig7.png", width = 1200, height = 600, res = 144)
ggplot(fig7, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Germany") +
    ylab(label = "Mean Support") +
    scale_color_manual(labels = c("M: Capital Accum.", "M: Low Srvc.", "F: Capital Accum.", "F: Low Srvc."), values = c("red", "green", "red3", "green3"))
invisible(dev.off())
knitr::include_graphics("Fig7.png")


# Fig 8 

fig8 <- subset(fig2g, cntry == 826)
fig8$sh_groups_new <- as.factor(fig8$sh_groups_new)
fig8 <- fig8[which(fig8$sh_groups_new %in% c(1,5,6,10)),]

agg_png("Fig8.png", width = 1200, height = 600, res = 144)
ggplot(fig8, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Great Britain") +
    ylab(label = "Mean Support") +
    scale_color_manual(labels = c("M: Capital Accum.", "M: Low Srvc.", "F: Capital Accum.", "F: Low Srvc."), values = c("red", "green", "red3", "green3"))
invisible(dev.off())
knitr::include_graphics("Fig8.png")

```

### Welfare State and Redistributive Prefs

Decommodification data from CWED2
Government Consumption Data from OECD
Government Spending Data from OECD
Pre- Post- Gini from Solt

```{r macro_data}
# get from github - cwed
pacman::p_load(readr)
cwed <- read.csv(paste(wd, "cwed-subset.csv", sep = "/"))

cwed$cntry <- countrycode(cwed$COUNTRY, "country.name", "iso3n")
cwed$year <- cwed$YEAR

cweds <- dplyr::select(cwed, cntry, year, TOTGEN, UEGEN, SKGEN, PGEN)

# Get other data later




```


```{r macro_table}
tab_macro <- fig2 %>%
  mutate(year2 = year,
         year = ifelse(year > 2010, 2010, ifelse(year < 1987, 1987, year))
  )

tab_macro <- left_join(tab_macro,cweds, by = c("cntry","year"))

tab_macro$year <- tab_macro$year2
tab_macro <- dplyr::select(tab_macro, cntry, year, sh_groups_new, TOTGEN, reduceM, reduceSE)
tab_macro <- tab_macro[order(tab_macro$cntry, tab_macro$year),]
tab_macro <- subset(tab_macro, !is.na(tab_macro$TOTGEN))

# get aggregate scores by country/year
# mean
t_mean <- aggregate(tab_macro, by = list(tab_macro$cntry, tab_macro$year), FUN = mean)
t_min <- aggregate(tab_macro, by = list(tab_macro$cntry, tab_macro$year), FUN = min)
t_max <- aggregate(tab_macro, by = list(tab_macro$cntry, tab_macro$year), FUN = max)

# clean
t_mean <- dplyr::select(t_mean, cntry, year, TOTGEN, reduceM, reduceSE)

t_min$min <- t_min$reduceM
t_min <- dplyr::select(t_min, cntry, year, min)

t_max$max <- t_max$reduceM
t_max <- dplyr::select(t_max, cntry, year, max)

t_max <- left_join(t_max, t_min, by = c("cntry", "year"))
t_max$range <- t_max$max - t_max$min
t_max <- dplyr::select(t_max, cntry, year, range)

tab_macro <- left_join(t_mean, t_max, by = c("cntry","year"))

tab_macro <- tab_macro[order(tab_macro$TOTGEN, tab_macro$cntry, tab_macro$year),]


tab1_m <- kable(tab_macro)

kable_styling(tab1_m)


```


```{r scatter_macro}
# create regime variable

tab_macro <- tab_macro %>%
  mutate(regime = case_when(
    cntry == 752 | cntry == 208 | cntry == 578 | cntry == 246 ~ 1,
    cntry == 276 | cntry == 250 | cntry == 56 | cntry == 528 | cntry == 40 | cntry == 756 ~ 2,
    cntry == 724 | cntry == 620 | cntry == 380 ~ 3,
    cntry == 392 | cntry == 410 ~ 4,
    cntry == 36 | cntry == 840 | cntry == 554 | cntry == 826 | cntry == 372 | cntry == 124 ~ 5
  ))

tab_macro$regime <- as.factor(tab_macro$regime)
agg_png("Fig4.png", width = 1000, height = 600, res = 144)
ggplot(tab_macro, aes(y = TOTGEN, x = range, color = regime)) +
  geom_point() +
  scale_color_manual(labels = c("Scandinavia", "Cntl. Europe", "S. Europe", "East Asia", "English Diaspora"), values = c("red", "green3", "blue", "darkorange",  "darkgreen"))
invisible(dev.off())
knitr::include_graphics("Fig4.png")

```


```{r}
ggplot(tab_macro, aes(y = TOTGEN, x = range)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE)
```











### Risk and Redistributive Preferences

#### Table XX. Risk by Group (a la S&H)

```{r risk_prefs}

# make risk-based categorizations

df <- df %>%
  group_by(cyear) %>%
  mutate(atrisk = ifelse(wk_unemp == 1 | wk_lesspart == 1, 1, 0),
    risk_cymean = mean(atrisk, na.rm = T))

df <- df %>%
  group_by(cyear, sh_groups) %>%
  mutate(risk_gcymean = mean(atrisk, na.rm =T),
         risk_gcyse = (sd(atrisk, na.rm=T)/sqrt(length(which(!is.na(atrisk))))),
         risk_grp_rate = risk_gcymean - risk_cymean,
         count = sum(!is.na(sh_groups))) %>%
  ungroup()



tab3 <- dplyr::select(df, sh_groups, cntry, year, reduce, risk_grp_rate, risk_gcyse, count)
tab3 <- subset(tab3, !is.na(tab3$sh_groups))
tab3 <- aggregate(tab3, by = list(tab3$cntry, tab3$year, tab3$sh_groups), FUN=mean, na.rm=T)
tab3 <- tab3[-c(1:3)]
tab3 <- tab3 %>%
  mutate(risk_grp_rate = ifelse(risk_grp_rate == "NaN", NA, round(risk_grp_rate, 3)),
         risk_gcyse = ifelse(risk_gcyse=="NaN", NA, ifelse(risk_gcyse!=0, round(risk_gcyse,3), 0.001)))

tab3de <- subset(tab3, tab3$cntry==276)

# Make DE-specific table later
```

```{r risk_p_corr}
tab3simp <- subset(tab3, tab3$cntry==276 | tab3$cntry==826 | tab3$cntry==752 | tab3$cntry==616)
ggplot(tab3simp, aes(x=risk_grp_rate, y=reduce, color=factor(cntry))) +
  geom_point(size=2) +
  geom_smooth(aes(x=risk_grp_rate, y=reduce), method=lm, se=FALSE)

```


### Fig 3. Great Britain

```{r fig3}
fig2gb <- subset(fig2, cntry == 826)
fig2gb$sh_groups_new <- as.factor(fig2gb$sh_groups_new)

ggplot(fig2gb, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Gr. Britain") +
    ylab(label = "Mean Support") +
    scale_color_hue(labels = c("Capital Accum.", "Mixed Srvc.", "Blue-collar/low", "Soc.Cult. Prof.", "Low Srvc."))
```


### Fig 4. United States


```{r fig4}
fig2us <- subset(fig2, cntry == 840)
fig2us$sh_groups_new <- as.factor(fig2us$sh_groups_new)

ggplot(fig2us, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, United States") +
    ylab(label = "Mean Support") +
    scale_color_hue(labels = c("Capital Accum.", "Mixed Srvc.", "Blue-collar/low", "Soc.Cult. Prof.", "Low Srvc."))
```



```{r income_analysis}
m1 <- lm(reduce ~ as.factor(sh_groups_new)*hincZ*female, data = df)

summary(m1, scale = T)


df_g1 <- df[df$sh_groups_new == 1, ]

df_g1$rincZ <- scale(df_g1$rincZ)
df_g1$rincZ <- as.vector(df_g1$rincZ)
df_g1$reduceZ <- scale(df_g1$reduce)
df_g1$reduceZ <- as.vector(df_g1$reduceZ)


m2 <- lm(reduceZ ~ rincZ + female + rincZ*female, data = df_g1)

summary(m2, scale = T)
summary(m2)

df$age2 <- df$age^2
m3 <- lm(reduce ~ rincZ + female + degree + age2 + age, data = df[df$cntry == 124, ])

summary(m3)

m4 <- lm (reduce ~ female*age*as.factor(sh_groups_new), data = df)

summary(m4)

ggplot(fig2gde, aes(x=age, y=reduceM, color=sh_groups_new))+
    geom_point(size=2) +
    geom_line()
```
1 "Capital accumulators" 
2 "Mixed service functionaires" 
3 "Blue and lower white collar" 
4 "Socio-cultural (semi-)professionals" 
5 "Low service functionaires"

```{r ineraction_f_inc}
plot_model(m1, type = "pred", terms = c("hincZ", "sh_groups_new[1,3,4,5]", "female"))

plot_model(m2, type = "pred", terms = c("rincZ", "female"))
```




```{r corrs}
tab3 <- dplyr::select(df_g1, reduce, rincZ, female)
cor2 <- cor(tab3, use = "pairwise.complete.obs")
kable(cor2, digits = 2)


```















## Different Approaches to Group Comparison

Three types of comparison:
1. Null Hypothesis - Least Squared - Line of best fit
2. Testing an Assumption - two groups are equal
3. Bayesian Estimation - posterior probability/distribution


### 1. NHST - Null Hypothesis Significance Testing

We want to investigate if two groups are different. The null hypothesis is that they are not different. We have a linear model like such:

$Y_{gincdiff} = a_{intercept (reference-category)} + X_{1.poverty} + X_{2.blue-collar} + X_{4.others} + e$

In this example the 'line-of-best-fit' will explain the average level of support for redistribution in each group. The null hypothesis is that the three $X$ groups are not different from the omitted category $X_{3.high-status}$

The error term $e$ is the difference between the predicted values and the observed values in the data.

Here we look at the case of Germany 2010.

```{r nhsta}


de10 <- subset(df, year == 2010 & cntry == 276)

# remove all missing values for now

de10 <- subset(de10, !is.na(reduce))
de10$groups <- as.factor(de10$sh_groups_new)

# estimate means across groups ('lines-of-best-fit') - OLS

de10$groups <- relevel(de10$groups, ref = 3)

m1 <- lm(reduce ~ groups, data = de10)

summary(m1)


```


### Alternative Hypothesis Testing - Wald Tests

We can also compare the groups to a pre-specified value, like the sample mean. These are known as Likelihood Ratio and Wald tests

```{r m1_wald}

# get sample mean

reduceM <- mean(de10$reduce)

# calculate test statistic, it will be the sample mean minus the intercept from m1

reduce_test <- reduceM - m1$coefficients[[1]]

# test that poverty group is sign different from sample mean
linearHypothesis(m1, c(paste0("groups2 =", reduce_test)))

```

```{r m1_wald2}

# test that both poverty and blue-collar group are significantly dif from sample mean
linearHypothesis(m1, c(paste0("groups1 =", reduce_test), paste0("groups2 =", reduce_test)))

```

```{r m1_wald2a}

# test that all three groups are different from sample mean 
linearHypothesis(m1, c(paste0("groups1 =", reduceM), paste0("groups2 =", reduce_test), paste0("groups4 =", reduce_test)))

```


### Constraints - Structural Equation Modeling


#### M2. Baseline 'free' model.

Maximum likelihood (ML) estimation as opposed to OLS. They are similar when residuals are normally distributed. However, in the presence of heteroskedasticity (when error variance differs markedly across levels fo predictor variables) they diverge. ML often becomes prefereable in these cases. ML uses the variance of the sample. If it is skewed (leading to skewed residuals that is) then ML iterates (its guesses) until it finds a distribution that is most likely the source of the data-generating process that led to observation of these values. (some reading on ML https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1_)

In our simple example this just doesn't matter much, but as we add more variables it probably does.


```{r m2_sem}
# lavaan does not accept factor variables so we have to recode manually

de10 <- de10 %>%
  mutate(group1 = ifelse(sh_groups_new == 1, 1, 0),
         group2 = ifelse(sh_groups_new == 2, 1, 0),
         group3 = ifelse(sh_groups_new == 3, 1, 0),
         group4 = ifelse(sh_groups_new == 4, 1, 0)
  )

# run a simple linear model

m2 <- 'reduce ~ group1 + group2 + group4'

m2 <- sem(m2, data = de10)

summary(m2)

# notice that there are 'iterations' taking place until the ML estimate is maximized.
```

#### M3. Equality Constraints Across Groups

In SEM we can design models that force groups to be equal (i.e. make a hypothesis that in the real world, the people from whom we sampled these data have the same (distribution of) attitudes toward redistribution (i.e., same mean (and if we want, variance)))

Here the 'a' is the name we give to a parameter to make groups equal. As we want these three groups to be equal and do not care about the group 'other' we now put these three groups into the model as variables. Doing this does not change anything about the fit of the model, but if we were to omit group3 as we have so far, we would be allowing group3 to be different form groups 1,2 & 4 which is not what we want. 

```{r m3_sem}


m3 <- 'reduce ~ a*group1 + 0.239*group2 + a*group3'

m3 <- sem(m3, data = de10)

summary(m3)

# notice all groups now have identical coefficients
```


By constraining the coefficients we are certain to get predicted values that are not quite as good at maximizing fit as if we let each group be freely estimated. The question now becomes how do these predicted values now look when the equality constraints are applied. We can check this in several ways.  When we apply the equality constraints, we are forcing the correlation matrix to change (sometimes only slightly and other times quite a lot). If we have to change the matrix dramatically we are probably not producing an accurate data-generating model. 

#### Looking at Observed versus Implied Correlations


##### Observed Covariances - in the data
```{r m3_ocov}

# first extract only those variables we want to see
de10s <- dplyr::select(de10, reduce, group1, group2, group3, group4)
m3cov <- round(cov(de10s, use = "pairwise.complete.obs"), 3)

```


##### Implied Covariances - as a result of our equality constraints

```{r m3_cov}
m3covI <- as.matrix(fitted(m3))

tab_m3 <- rbind(m3cov, unlist(m3covI))

kable(tab_m3)

```


#### Testing Whether the two covariance matrixes are significantly different

```{r m3_chitest}
compareFit(m2, m3)
```

### Hung's corner

```{r hung}


# Bayesian estimation - ANOVA

## non-informative prior 
m1 <- lm(reduce ~ as.factor(sh_groups_new), data = de10)
summary(m1)

## The anova() function in R compares variability of observations between the treatment groups to variability within the treatment groups to test whether all means are equal or whether at least one is different. The small p-value here suggests that the means are not all equal.

anova(m1)


de <- subset(df, cntry == 276)

de <- de[complete.cases(de$sh_groups_new),]  # get complete cases for the model to work

## Model 
mod_string = " model {
  for (i in 1:length(y)) {
      y[i] ~ dnorm(mu[grp[i]], prec)
  }
  for (j in 1:5) {
      mu[j] ~ dnorm(0.0, 1.0/1.0e6)
  }
  prec ~ dgamma(5/2.0, 5*1.0/2.0)
  sig = sqrt( 1.0 / prec )
}"

set.seed(62)

data_jags = list(y = de10$reduce, grp = as.numeric(de10$sh_groups_new))
params = c("mu", "sig")

inits = function() {
  inits = list("mu" = rnorm(5,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod = jags.model(textConnection(mod_string), data=data_jags, inits = inits, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod, variable.names=params, n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))

## Convergence checks 


gelman.diag(mod_sim)
autocorr.diag(mod_sim)
effectiveSize(mod_sim)


(pm_params = colMeans(mod_csim))

yhat = pm_params[1:5][data_jags$grp]
resid = data_jags$y - yhat

plot(yhat, resid)


## Results

summary(mod_sim)
HPDinterval(mod_csim)

mean(mod_csim[,3] > mod_csim[,3])

## Conclusion: Seems like these groups are significantly different from each other 

```

Let's try a more complicated model using Bayesian estimation, this time an Ordinal Logistic/Probit Regression


```{r OLR}

## Model setup - ordinal logit

mod_string2 <- " model {
  for (i in 1:N) {
    y[i] ~ dcat(p[i,])
    p[i,1] <- 1 - Q[i,1]
    for (r in 2:4) {
      p[i,r] <- Q[i,r-1] - Q[i,r]
    }
    p[i,5] <- Q[i,4]
    for (r in 1:4) {
      logit(Q[i,r]) <- b1*x[i] - c[r]
    }
  }
  for (i in 1:4) {dc[i] ~ dunif(0, 20)}
    c[1] <- dc[1]
  for (i in 2:4) {
    c[i] <- c[i-1] + dc[i]
  }
  b1 ~ dnorm(0, 0.001)
  or <- exp(b1)
}"



data_jags2 = list(y = as.factor(de10$reduce), x = as.factor(de10$sh_groups_new), N = nrow(de10))
params2 = c("b1", "or","c" )

mod2 <- jags.model(textConnection(mod_string2), data = data_jags2, n.chains = 3) # 3 chains
## Run the model 
update(mod2, 1e3)  # Burn-in phase, 1000 iterations

mod_sim2 <- coda.samples(model=mod2, variable.names=params2, n.iter=5e3) # 5000 iterations for each chain

mod_csim2 <- as.mcmc(do.call(rbind, mod_sim2)) # merge the chains

## Convergence checks
plot(mod_sim2)

gelman.diag(mod_sim2)
autocorr.diag(mod_sim2)
effectiveSize(mod_sim2)


(pm_params2 = colMeans(mod_csim2))

## Results

summary(mod_sim2)



# ordered probit regression 

## reduce ~ sh_groups_new * female



mod_string3 <- " model {
  for (i in 1:N) {
    y[i] ~ dcat(p[i,1:5])
    p[i,1] <- pnorm(c[1], mu[i], 1/sigma^2)
    for (r in 2:4) {
      p[i,r] <- max(0, pnorm(c[r], mu[i], 1/sigma^2) - pnorm(c[r-1], mu[i], 1/sigma^2))
    }
    p[i,5] <- 1 - pnorm(c[4], mu[i], 1/sigma^2)
    mu[i] <- zbeta1 + zbeta[2]*zx1[i] + zbeta[3]*zx2[i] + zbeta[4]*zx1[i]*zx2[i]
  }
  zbeta1 ~ dnorm(6/2, 1/(5^2))
  for (j in 2:4) {
    zbeta[j] ~ dnorm(0,1/(5^2))
  }
  sigma ~ dunif(5/1000,5*10)
  for (i in 2:3) {c[i] ~ dnorm(i+0.5, 1/2^2)} #c[1] and c[4] are defined below
}"

c = rep(NA,4)
c[1] = 1 + 0.5
c[4] = 4 + 0.5


# df_new <- df[complete.cases(df$sh_groups_new),]
# df_new <- df_new[complete.cases(df_new$reduce),]


data_jags3 <- list(y = as.factor(de10$reduce), zx1 = as.factor(de10$sh_groups_new), zx2 = de10$female, N = nrow(de10), c = c)

mod3 <- jags.model(textConnection(mod_string3), data = data_jags3, n.chains=3)

params3 = c("zbeta1", "zbeta", "sigma", "c")

update(mod3,1e3)

mod_sim3 <- coda.samples(model=mod3, variable.names=params3, n.iter=5e3)

summary(mod_sim3) #zbeta in sd units (according to Gary King?)

mod_sim3
mod_csim3 = as.mcmc(do.call(rbind, mod_sim3))
(pm_params3 = colMeans(mod_csim3))
mod_csim3 <- as.data.frame(mod_csim3)


#Still working on transformation to an interpretable scale

beta <- setNames(data.frame(matrix(ncol=4,nrow=1)),c("beta1","beta2","beta3","beta4"))

beta$beta1 <- mean(mod_csim[,9]) - sum() 
beta$beta2 <- mean(mod_csim3[,6])/sd(mod_csim3[,6])
beta$beta3 <- mean(mod_csim3[,7])/sd(mod_csim3[,7])
beta$beta4 <- mean(mod_csim3[,8])/sd(mod_csim3[,8])

```

```{r espanet}

fig5 <- df %>%
  mutate(ageSH = ifelse(ageC < 40, 0, 1)) %>%
  group_by(cyear, sh_groups_new, ageSH) %>%
    mutate(reduceM = mean(reduce, na.rm = T),
           reduceSE = (sd(reduce, na.rm = T)/sqrt(length(which(!is.na(reduce)))))) %>%
    dplyr::select(year, cyear, reduceM, reduceSE, wave, cntry, sh_groups_new, female, ageSH)

fig5$sh_groups_new <- fig5$sh_groups_new + (5*fig5$ageSH)
fig5 <- subset(fig5, select = -c(female,ageSH))

fig5 <- aggregate(fig5, by=list(fig5$cyear,fig5$sh_groups_new), FUN=mean, na.rm=T)

fig5de <- subset(fig5, cntry == 276)
fig5de$sh_groups_new <- as.factor(fig5de$sh_groups_new)
fig5de <- fig5de[complete.cases(fig5de$sh_groups_new),]
fig5de <- fig5de[which(fig2gde$sh_groups_new %in% c(1,5,6,10)),]

# Fig 5 - Run in console
agg_png("Fig5.png", width = 1200, height = 600, res = 144)
ggplot(fig5de, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Germany") +
    ylab(label = "Mean Support") +
    scale_color_manual(labels = c("Under 40: Capital Accum.", "Under 40: Low Srvc.", "Above 40: Capital Accum.", "Above 40: Low Srvc."), values = c("red", "green", "red3", "green3"))
invisible(dev.off()) 
knitr::include_graphics("Fig5.png")



# Fig 6 - Run in console


fig6gb <- subset(fig5, cntry == 840)
fig6gb <- fig6gb[complete.cases(fig6gb$sh_groups_new),]
fig6gb$sh_groups_new <- as.factor(fig6gb$sh_groups_new)
fig6gb <- fig6gb[which(fig6gb$sh_groups_new %in% c(1,5,6,10)),]



agg_png("Fig6.png", width = 1200, height = 600, res = 144)
ggplot(fig6gb, aes(x=year, y=reduceM, color=sh_groups_new)) + 
    geom_errorbar(aes(ymin=reduceM-reduceSE, ymax=reduceM+reduceSE), colour="grey", width = .15) +
    geom_point(size=2) +
    geom_line() +
    labs(title = "Support for Redistribution Over Time, Great Britain") +
    ylab(label = "Mean Support") +
    scale_color_manual(labels = c("Under 40: Capital Accum.", "Under 40: Low Srvc.", "Above 40: Capital Accum.", "Above 40: Low Srvc."), values = c("red", "green", "red3", "green3"))

invisible(dev.off()) # optional
knitr::include_graphics("Fig6.png")






# Analyses
df_new <- df %>%
  mutate(ageSH = ifelse(ageC < 40, 0, 1))

df_new <- df_new[complete.cases(df_new$ageSH),]
de <- subset(df_new, cntry == 276)
gb <- subset(df_new, cntry == 840)
# LM
m1 <- lm(reduce ~ as.factor(sh_groups_new) + female, data = de)
summary(m1)
m2 <- lm(reduce ~ as.factor(sh_groups_new) + ageSH, data = de)
summary(m2)
m3 <- lm(reduce ~ as.factor(sh_groups_new)*female + ageSH + educyrs, data = de)
summary(m3)


m4 <- lm(reduce ~ as.factor(sh_groups_new) + female, data = gb)
summary(m4)
m5 <- lm(reduce ~ as.factor(sh_groups_new) + ageSH, data = gb)
summary(m5)
m6 <- lm(reduce ~ as.factor(sh_groups_new)*female + ageSH + educyrs, data = gb)
summary(m6)


m7 <- lm(reduce ~ as.factor(sh_groups_new) + female, data = df_new)
summary(m7)
m8 <- lm(reduce ~ as.factor(sh_groups_new) + ageSH, data = df_new)
summary(m8)
m9 <- lm(reduce ~ as.factor(sh_groups_new)*female + ageSH + educyrs, data = df_new)
summary(m9)





library(sjlabelled)
library(sjmisc)
library(sjPlot)

tab_model(m1, m2, m3, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3"), pred.labels = c("Mixed Service", "Blue collar/Low White", "Semi-Professional", "Low Serivce", "Female", "Over 40", "Education years", "MS*Female", "BC*Female","SP*Female", "LS*Female") ,file = "Table2.html")

tab_model(m4, m5, m6, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3"), pred.labels = c("Mixed Service", "Blue collar/Low White", "Semi-Professional", "Low Serivce", "Female", "Over 40", "Education years", "MS*Female", "BC*Female","SP*Female", "LS*Female") ,file = "Table3.html")

tab_model(m7, m8, m9, p.style = "stars", p.threshold = c(0.10, 0.05, 0.01), show.ci = F, rm.terms = c("(Intercept)"), show.loglik = T, show.aic = T, dv.labels = c("M1", "M2","M3"), pred.labels = c("Mixed Service", "Blue collar/Low White", "Semi-Professional", "Low Serivce", "Female", "Over 40", "Education years", "MS*Female", "BC*Female","SP*Female", "LS*Female") ,file = "Table4.html")


```





